This is the raw output from me feeding the draft into Dolphin 2.2.1 Mixtral Instruct, with no modifications except re-adding some markdown formatting. My final version posted to my blog was a revised version of this - fixing a few AI mistakes, a few mistakes AI missed, and some more minor revisions to better express what I intended. Prompt: "Please revise the following draft, fixing only grammatical mistakes and spelling without changing the tone or meaning:"

What you are reading now is the fourth version of this post that has gone through multiple revisions and considerations. It might feel a bit fragmented, but my aim is to provide a comprehensive post covering two related topics. The first part will discuss my experimentation with local LLMs (large language models), and the second will explore my personal philosophy and conclusions on AI. Feel free to only read one or the other. They could have been separate posts, but I enjoy writing (and reading) long posts that are well thought out and cover a wide range of topics. If you regularly read my writings, you know that I have a habit of writing posts that expand beyond what I initially intended.

# Part 1: LLM Experimentation
As of lately, I've been experimenting with large language models on my computer, and it has been quite interesting. So far, I have tried out four different AI models: Mixtral 7B Instruct v0.2 at 4 or 5 QBits, StableLLM Zepher at 2QBits, Dolphin Phi 2 at 6 QBits, and Dolphin 2.2.1 Mixtral Instruct at 5QBits. I am also looking forward to playing with Google Gemma, although at the time of my third draft of this post, it is incompatible with the latest version of LM Studio for Linux.

---

### What I've Been Using
Everything that I have tested locally so far has been done through LM Studio, a user-friendly software that works on Windows, Mac, and Linux. It is completely free to use, and even though I am accustomed to using the command line, one click to download and another click to run has been quite convenient. I am using mid-range hardware with an Nvidia 1650 (4GB VRAM) and 16GB RAM, so if your computer is similar or exceeds mine in those specifications, you can run anything I have tested.

In case you are unable to run a local LLM or do not wish to set one up yourself, there are two simple alternatives. The Brave browser (desktop and mobile) offers free access to the Lama, Mixtral 7b, and Claude models without registration. Alternatively, you can usually find free-to-run models on Hugging Face, either through a search engine or by finding a specific model and locating the services people have set up with it.

So, without further ado, here are the four models I primarily use:
**Mixtral 7B Instruct v0.2** is a decent all-round LLM developed by Mixtral that can perform various basic tasks and runs well on lightweight hardware. If you are looking to run a fairly stable LLM locally with decent output quality, this would likely be the first thing you should try.
**StableLLM Zepher**, particularly the 2QBits version I used, is very lightweight at the cost of output quality. While the quantized Mixtral model experiences more hallucinations (e.g., thinking each month gets an extra day during a leap year), Zepher does not seem to do so as much and instead produces less verbose output.
**Dolphin Phi 2** is a modified version of Phi 2 designed to be "uncensored". Phi is also quite lightweight, making it suitable for a wide range of hardware configurations.
**Dolphin 2.2.1 Mixtral Instruct** is an "uncensored" version of Mixtral Instruct that also appears to provide slightly improved accuracy compared to the standard model (for example, despite having the same size, it does not exhibit the leap year issue mentioned above). So far, it has been my go-to choice for general LLM tasks.

---


## Successes and failures

#### Composing Content
As far as composing content goes, all the models I've used output the generic AI sludge. Tell it to write a blog post about trees and you get a short blog post that looks like a robot is pretending to be a bureaucrat. If you give it a more complex prompt, such as an outline and all the topics you want it to cover, it gets a little better but still has the AI feel to it. If you were doing something generic and didn't like writing (say asking it to write a quote based on information you provided) or were just looking for inspiration, it might be handy, but I really don't have a use for generating written content so I can't really judge its success in specific situations I'd want to use it.

#### Proofreading
I had a lot of luck having LLMs proofread my text, although it did require a bit of fine-tuning my prompts. If I just told it to proofread my writings, it had a habit of rewriting them in standard AI sludge tone, but after fine-tuning my prompt *and* reproofreading the LLM proofread version to check for any mistakes it made, it's actually doing pretty good. As you might have guessed, this post was proofread with AI, specifically Dolphin 2.2.1. You're reading the draft, run through the final prompt, and then re-proofread by me.

Original draft without spellcheck or grammar assistance

Prompt: Please revise this blog post

Prompt: Please proofread this blog post

Prompt: Please proofread this blog post only fixing major grammatical errors, spelling mistakes, and punctuation without changing the tone or meaning

I am pretty satisfied with how it proofreads. I usually proofread my content with the free version of Grammarly, but I could see myself using a local LLM more regularly in the future. Ironically, Grammarly will use my writing to train AI models, but running a local LLM my writing never leaves my computer.

#### Summarization
Summarization has been a mixed bag. All models except Zepher seemed to do a pretty good job most of the time at summarizing content, listing off the important details but cutting down the amount of text. However, there have been issues with hallucinations. For example, when I asked Mixtral 7B to summarize my blog post on microblogging protocols, it stated that I said Nostr runs on the Bitcoin blockchain. Not only is that untrue, as Nostr is not built on Bitcoin (or any crypto for that matter) or a blockchain, but neither the words "bitcoin" nor "blockchain" even appeared in the post. It works well most of the time, but I certainly wouldn't be 100% trusting of the output as it does make occasional mistakes.

Beyond just basic summarization, the various LLMs also do a good job answering questions about content. For example, using my comparison post, I was able to ask questions such as "what did the author think about Nostr's ability to scale?" "what does the author think BlueSky should do?" and "what is the author's favorite protocol?" each time it replied with well-explained answers without any errors.

#### Translation
Something that really surprised me is that local LLMs appear to excel at translating content. After experimenting, I had a lot of luck with both versions of Mixtral listed above, translating to and from a handful of different languages with nearly as good of a result as Google Translate. This could be useful for doing things like reading blog posts in other languages that have been cached in an RSS reader or translating a downloaded document. Paired with something like Tesseract (optical character recognition) or Whisper AI (AI audio/video transcription), it could be used in a wide variety of circumstances.

#### Writing Code
Writing code was both a success and a failure depending on how you want to look at it. Mixtral 7B, the local version with 4qb, and the Brave unquantized version all ended up helping me write my [multi-protocol python client]. I had the idea of building the multi-protocol client because I thought software like that should exist, but I'm not a programmer or developer, so I figured I would ask AI to try to write it for me. Every line of code it wrote failed spectacularly, but it did tell me about the library Pynostr, which let me know I could use somebody else's code as a base and actually build the client without needing much in terms of development skills. It also helped explain to me exactly what I needed to publish my client into the Pip repos - so despite every line of code being written by me, Mixtral was pretty instrumental in helping me write the code.

In this sense, I would consider it quite successful. It can't write functional code for something new or even something that exists but is fairly niche, but it did a good job at explaining concepts and nudging me in the right direction. As it stands now, I would probably say that the local LLMs I used were a minor upgrade from having an archive of Stack Exchange with Kiwix: incredibly useful to extend your capabilities, but incredibly useless to do your work for you.

#### Using an LLM for Offline Information Storage/Search:
Something else that surprised me is how much information can be stored offline and run locally. All the models I've used are under 6GB, and yet you can ask it just about anything and expect a decent answer. For example, to toy around and test its capabilities, I asked it how to cook steak; then wound up in a deep conversation about things like marbling and what the USDA cut grades mean - and having previously worked at a grocery store's meat department, I was able to verify that everything it was giving me was correct and supplied in a useful manner. I still wouldn't trust it to provide me with important information without first verifying things, but the idea of having something that could supplement a search engine or tutorials all in a few GBs entirely offline is pretty immense. And this is coming from somebody who has a hard drive with massive amounts of data archived on it, such as the entire Project Gutenberg catalog, Simple English Wikipedia, textbooks, and similar archives.

---

## Other Concepts

#### How an LLM Works
While I am discussing other topics, I might as well debunk some myths about large language models. I keep hearing people say things like ChatGPT will give us nuclear fusion, and of course the one Google employee thought his AI became sentient because it said so in a text chat. While I cannot speak with certainty about proprietary information or my expertise in AI, the gist is that LLMs predict text.

Let's illustrate this concept. If you have a Python script containing `print("Hello World")`, it will output the text "Hello World". Now let's say you have never even heard of Python; all you know is what I just told you. Then, if I ask you to write me a script that says "Goodbye World", you can probably figure that out quite easily and so could an LLM. However, if I were to ask either you or the AI to create a calculator in Python or even describe what Python is, then both you and the AI would likely be at a loss.

A large language model (LLM) is essentially a tool that has been trained on vast amounts of data. When presented with a prompt, it predicts the words it believes are most likely to follow based on this data. It does not have much more understanding of what's going on than a parrot repeating a series of sounds to get a cracker; it is only humans that can extract meaning from phrases like "polly wants a cracker".

#### "Censored" and "Uncensored" AIs
This is a bit of a touchy topic, but it's worth covering when talking about local LLMs since you can generally remove the safeguards in place. There are some cases where LLMs are "censored" to fit the ideologies of their creators, but in most cases it's just a bunch of safeguards to keep the chatbot from saying something bad that would negatively reflect on somebody. For example, if Ford pays to have a chatbot compare various vehicles they really don't want screenshots of people having the Ford chatbot tell you how to stab someone or make illicit substances. Local LLMs, and even (in my opinion) your run-of-the-mill standalone ChatGPT, don't/shouldn't have that issue. To paraphrase Richard Hoeg quoting 2001: A Space Odyssey, having an "I'm sorry Dave, I can't do that" moment on your own computer (or a service you're paying for) when you're asking for specific information from your own device shouldn't happen.

Let's use information on how nuclear weapons are made as an example. It's pretty absurd to think any of you are going to be making nuclear weapons, so if somebody is asking how to make a nuclear weapon then they're probably not actually intending to do so. Besides, even if Kim Jong Un is somehow reading this blog or using a chatbot, there's better information on how to do so on Wikipedia than here or on my chatbot. However, if you ask Mistral 7B how to make a nuclear weapon it refuses, as pictured below:

Pretty pointless if you ask me. Sure Ford might not want their chatbot to tell you, but outside of that it's pretty pointless to refuse to tell you how to make a nuclear weapon - especially when, again, you can find the basics on Wikipedia. But the great thing about libre software is you can modify it, which is exactly what Dolphin 2.2.1 Mixtral Instruct is, a version of Mixtral with the safeguards taken off. Again, asking it the same question, and a more specific follow-up question:

In all reality, whether safeguards have been removed or not it will likely not affect you one way or the other in a majority of your queries. Still, it may be useful occasionally, and more importantly there's just the idea that I want my computer to answer to me and not somebody else.

#### Future of Local LLM Usage
So, where do I see large language models (LLMs), especially local ones, in the future? Well, the biggest trend I see happening is a lot of AI processing AI data, such as someone writing a long email from a bullet point outline using an LLM and then another person having their LLM summarize that email in bullet points.

Outside of that, I do expect AI to take the place of personal assistant type apps and tools, both locally and remotely. I can assume that in the future, LLMs will only get more space/resource efficient, become more reliable with the information they spit out, and more easily setup by less technical users. I can see the creation of an app with an interface/APIs that allow other apps to communicate with it to send in data to be processed and then receive output from it. If it's for a remote service like ChatGPT, the app would just send the query out to ChatGPT's servers to be processed and sent back. If it's a local LLM installed by the user, the app could throw the 1-6GB LLM model into RAM and run it locally, then send back the data that was processed. Temporarily loading a couple of Gigs of data into RAM and then processing it using integrated or dedicated graphics is already possible on most mid-upper level phones and most laptops/desktops, and with Google Gemma already having versions under 1GB, it's already possible on most devices.

As you've gathered from the post, all this is already possible if you're slightly techy, for example by just copy-pasting the text of an article and asking it to summarize, or telling it to translate the text in a picture you took by running the picture through Tesseract and putting the Tesseract output into the chatbot. Throw a little bit of accessibility paint on it, and next thing you know, you can send an article on your phone's browser to your default LLM app to be summarized, or hold your home button to ask a question, and have it processed either locally or by a LLM host without any technical knowledge required.

### Wrap Up
That about wraps up part one of my post. Even though it could probably have been a standalone post, I figured I'd combine that and my opinions on AI ethics into one big post. If you only came for this, and don't want to hear the millionth person give an unprompted opinion on new technology, feel free to close the page now.

Anyway, yeah, local LLMs seem pretty handy. Nothing groundbreaking compared to your run-of-the-mill assistant apps, but still an upgrade nonetheless, and something I have been using on and off as a utility even after the novelty of running one has worn off. My guess is that, given a little time to polish up the models and set up some sort of default API or other way for apps to communicate with an LLM on your device (or in the cloud), they'll become pretty widespread and able to process input from sources other than a chatbot app/web page.

---
---

# P2: Ethics
Okay, part two, my thoughts on AI in regards to ethics, laws, and what kind of effects they might have. This is definitly a very opiniony portion of the text, and I welcome discussion or disagreement (I'll share it on my socials if you want to respond, or you can respond via email if you'd like). For this I figure I'll just walk you through my thought process on the different things, keeping in mind I do have a decent understanding of technology, but am not a lawyer or AI expert.

The first and likely most significant controversy is training AI models using data read from the internet at random. Not all training procedures involve this method, but some do, resulting in criticism of AI models. While I might not be 100% confident about my stance on the issue, I ultimately believe it's not unethical to train AI models using this approach. From a technical standpoint: when an AI-training computer requests data from another person's computer or platform for a file copy, it is done willingly by the provider. As long as the AI does not distribute these copies to other individuals (which would be copyright infringement), the creator remains unaffected and the AI effectively performs similar tasks to those carried out by everyone else online (reading and learning without duplicating content). Even non-AI crawlers such as Google or Archive.org engage in this behavior, replicating and distributing content instead of simply training AI models with it.

In simpler terms, imagine a person handing out pamphlets about a cause on the street corner. They give them to anyone who asks (publicly available website). A professional pamphlet designer passes by wearing a shirt stating they're a designer (user agent string) and asks for a pamphlet (HTTP GET request), which is provided. Later, the pamphlet distributor realizes that their work might inspire the professional pamphlet designer, who could potentially make copies of their design without permission or compensation. The creator might argue that they should not have handed over the pamphlet in the first place, but since they already shared a copy with the designer, as long as the designer doesn't start duplicating the content (which is protected by copyright), they are still allowed to read and use elements from the pamphlet.

*Note: If you find yourself thinking, "I use x platform and I can't control where my content is distributed after uploading it," in that case, it's up to the platform since you generally grant them permission to distribute your content as they see fit. If you don't have control over a server, then it's the responsibility of the platform whether or not to give/sell your content.*

There are three responses I would expect to see from the above, and while there's really no right or wrong answers, I'd still like to explain how they fit into my thought process.

* The first is the issue of compensating the original author, but if somebody distributes their work for free then they've been compensated the exact amount they asked for. If this blog post inspires you to write a book that you sell I can't just come in and ask for payment after giving you a free copy.

* The next is the idea that, while AI won't (or at least shouldn't) re-produce anybody's work, it can only produce work after learning from other peoples' work - but of course good luck finding work produced by a human that isn't inspired by any other person's work.

* And finally, you might want to stop an AI from using your publicly accessible content. The simplest option is to set your robots.txt to disallow AI training crawlers (or any other crawlers that you don't want). Alternatively, it's possible to place your content behind a paywall or anything that requires registration and signing a terms of service.

Of course, all that aside, there are plenty of AIs that have been trained on licensed data. Stock photo owning companies have taken to training their own AIs on their own data, and there's a whole ecosystem built around funding platforms by collecting user data to be sold. Paid data sets are also probably much more controllable in terms of what goes in, so there's a good chance after all that (both all that writing I did and all that controversy that's gone on) the future of training AIs is going to be all licensed information anyway.

#### AI doing people stuff
Probably a confusing heading, but it's the best I could come up with. I've heard a lot of people say something along the lines of "well yeah, it's fine if a person did that, but AI is different." Usually it's used along the lines of something like quoting an article or learning data from a document or photo and then spitting out that data in a non-copyright infringing way. The justification is usually something along the lines of, "well an AI can do that at scale," but the idea never really resonates with me. First off, both with the ethics now and law stuff I'll touch on later, we only have the past to judge the future. If sharing a fact you learned elsewhere has always been fine up until this point, I just can't see how it suddenly isn't fine. And as far as doing things at scale suddenly changing it, if an AI learns a fact from an article and shares the fact with a thousand people, how different is that from the idea that somebody popular reads an article and shares a fact from it, and that fact is then distributed to a thousand people on Twitter?

#### AI Itself
There's no right or wrong answers to the above. However, I think that in some (a minority but not an insubstantial amount) of people against AI in its current form are against the concept of AI itself. There are two good examples that come to mind, the first of which is the backlash against Austin McOnnal for using AI. After he wrote a book (not with AI), he decided to do a promo where he animated the first chapter with AI, specifically with Adobe's AI that was trained on 100% licensed images. The heart of the work was made by a person and the AI was trained on licensed data, and despite that, he got a considerable amount of backlash. On a different note, I more recently listened to a several-hour-long very well put together piece on AI that had a somewhat negative opinion of it (sorry, I didn't save the link). Thing is, when it got to the point where the author went to say that AI's learning is unacceptable, his covering of the rebuttal was along the lines of *"And I know there are going to be people screaming in the comments section ""but facts and ideas can't be copyrighted,"" except AI isn't a person so it's different."* I was kind of looking forward to an argument against it, but that statement was nearly the entirety of what he covered on the topic and it ultimately only boiled down to 'this thing that's okay in other contexts is not in this one' without any elaboration as to why.

People may dislike the idea of generative AI itself for a variety of reasons, including but not limited to things such as a misunderstanding of technology works, the idea they dislike needing to compete with AI output in their particular field, or just a general dislike for something like that. New technologies are often disruptive and can create a lot of controversy when they first come out, and regardless of how many legitimate issues are resolved or taken into account, there will always be an initial backlash.

In 1906 John Philip Sousa stood before Congress arguing against recorded music. As an owner of a sheet music factory, he considered recorded music to be "canned music" that lacked soul. Live music performed by real people was an art form to be enjoyed, and recorded music produced by a machine was some form of abomination that was going to put musicians and sheet music makers out of business. Nowadays, some people still obsess over audio compression and headphone or speaker types to best replicate live music, and plenty of people pay thousands to experience live music, but the idea of recorded music being bad sounds absurd now that we've used the technology for generations. Recorded music brings a concert to any of us in any location, and the related technology is even used to enhance live music (e.g., speakers at a concert).

So, to quit with the simile, my thoughts on the use of various AI tools could be summarized as: they're new and disruptive, but once they find their place in the world, it's going to be normalized, and we're all going to move on. Being new and disruptive means they might cause trouble in the meantime, for people's jobs they disrupt or their misuse, and some of that will need to be dealt with. But in the end, it's just going to be another tool. There are a lot of legitimate conclusions to come to regarding many aspects (e.g., training policies), but AI as a whole is going to be here to stay, and soon enough most people probably won't give it a second thought.

### Future Effects
Speaking of their effects, honestly, I don't expect it to be as big as some people might think it'll be. Like I said above, I expect to see many emails written by an LLM using a short outline, and then summarized by the reader's LLMs. I'm expecting AI to do a lot of supporting work, proofreading documents, expanding upon or revising code, summarizing content in various forms, and similar stuff. Even for the seemingly more massive undertakings, it still will likely play second fiddle. For example, there was a comic book written using AI images recently. The story was written by a person, and the images were generated by AI and then edited and assembled by the person. This is how I expect things to be in the future as well: the important stuff - in this case the story itself, the character names and ideas, and the imagery themes - was written by a person, but an AI was able to offload a lot of work in a more supportive role like raw generating images.

This isn't to say there won't be any detrimental effects. Anything that makes people more efficient is going to lead to less jobs in the particular field, which while more effeciency is good for society in the long run it can obviously be detrimental for people who lose their jobs. From chatbots reducing the amount of people who need phone support to a jounalist writing faster because an AI was able to summarize and compare sources, there'll probably be a number of previous job openings slashed - especially if we're headed to an economic downturn and companies see an option to save a buck. There'll also probably be less money in more generic content, such as generating an AI image for an article instead of paying for stock image royalties. Beyond that, there's also the risk of incorrect information thanks to an AI messing something up while it assists people, like that lawyer who submitted non-existent citations after asking ChatGPT for citations and not checking them. And finally, there's always generic sludge content to be pushed out and clog up our news feeds and social media presence.

However, as much as there's plenty of good and bad to come, I don't even know if it'll be that big of a leap in technology. Consider writing code in three periods of time: pre-internet on older machines, a couple of years ago, and now with AI:

* Once upon a time, people were writing code in assembly language with nothing but books and memory to reference - not an easy feat to do.
* A couple of years ago you could pick from a million online tutorials to get you going, reference a million Stack Exchange answers (or ask your own question), only to then use pre-written libraries that effectively make writing simple code like using legos with the legwork done by those libraries.
* Now, compare that huge leap from early days to a few years ago, to the leap from a few years ago to today. Your computer can now read through those answers and give you a summary or produce basic code from that information.

Now, covering a more negative leap, consider the generic sludge content that everybody's worried about. In the olden days of the twenty teens all you needed to do was scrape a bunch of websites, run them through an article spinner (which replaces words with synonyms to trick search engines into thinking it's unique), and voila, you now had a bot that can pump out limitless sludge content to pull in search engine viewers. Again, AI is a % increase as prompting is slightly easier than scraping content and spinning it, but in all reality, it's nothing more than an expansion of an already existing thing. Even as a zoomer I can recognize that, of the jumps technology has made, AI hasn't been so much of a leap as it has been a step made after many other steps and leaps. You can take a look at any other field (say, publishing comic books) and probably see a similar trajectory that's been occurring.

*Besides, modern computers started with a command line to interact with, went to GUIs, and now we're going back to command lines of sorts again - just instead of simple commands we can type in plain English and the computer interprets that. It's inevitable: command line supremacy gang rise up!*

#### Laws and Stuff
Keep in mind I'm not a lawyer, and also keep in mind that even lawyers don't really know how everything will shake out. If Congress was less of a circus we might actually get modern laws, but for now any laws around AI are pretty much just coming court rulings and executive agency decisions. Still, there's some things that appear to be fairly straightforward, so I figured I would go over those. I also thought I would go over a few other things that are very much still up in the air as if I were a judge, taking it with a grain of salt.

First off, like I mentioned beforehand and in the paragraph above, **there's not a lot of AI-specific laws** on the books. This means all the laws governing normal copyright and computer behavior also apply to content made with AI. In a sense it's normal, most laws here in the west are derived from a long history of much older laws and concepts that have been built upon to fit modern circumstances.

As for copyright, it's important to note that **facts or ideas cannot be copyrighted**, only the expression of them can be. For example, if a person or LLM reads my blog post about IPFS, then states "*IPFS is a decentralized protocol that uses content based identification as opposed to location based identification*", that's not a violation of copyright. The only way you can commit copyright infringement is to actually reproduce my words themselves. Same goes for image generation, nobody can stop you from being inspired by a Van Gough painting and making one yourself using a similar style or color palette, only actually reproducing the work itself is copyright infringement.

As for training on public data, according to US legal cases such as *hiQ Labs v. LinkedIn*, **scraping content is generally legal** if you haven't agreed to not do so. If you have to sign up or otherwise agree to a terms of service then you are generally bound by it, but as long as the content is publicly accessible scraping is generally legal. Combine that with the fact that facts and ideas cannot be copyrighted, and most likely training content on public data is not copyright infringement under the current laws.

As for works generated by AI, in the US **in order for something to have copyright protections it needs to have a "spark of human creativity."** The Copyright Office has declined purely AI-designed content, so if you tell an image generator to make an image of a tree or an LLM to generate a blog post about trees then it's not likely to be protected by copyright. When paired with human-made content, the human-made portions of the content are still protected by copyright, for example, with the comic situation I mentioned above, the human-written story and the arrangement of the AI-generated images is protected but the images themselves are not protected. However, I expect that content co-authored by humans and AI will be copyrightable; for example, if you wrote an outline for a blog post about trees, told an LLM to write a post using the outline, then made edits to the output I expect that would be given normal protections. Nevertheless, the exact lines on what is and isn't protected are yet to be drawn.

Last, **what happens if an AI spits out work that infringes on copyright?** There's really no rulings or laws to cover this, so I'm just going to go over how I would rule if I were a judge. Say a person asks for a cartoon mouse, and an image generator generates something close enough to Micky Mouse that it's infringing on a copyright or trademark of some sort. First, I would say that since the program was not designed to infringe on copyright - and it doing so is the exception rather than the rule - the creators of the software shouldn't be held liable. Further, I would say accidental violation of copyright couldn't be held against the user of the program as they'd lack mens rea (intent to break laws), but if they intended to generate something that infringed copyright (or unintentionally generated an offending image but used it for other purposes anyway), then the user would be responsible for infringing copyright, just like they would be if they used Photoshop or a pencil to do so. But again, not a judge, lawyer, AI programmer, or someone who regularly generates content from scratch with AI.

*Quick note: In the case of ChatGPT, it likely has a lot of bells and whistles on top of it's basic LLM capabilities, which are likely responsible for regurgitating text and such. In the simple models that you would run locally, they're not actually storing data they've been trained on so they're probably not going to actually regurgitate text like ChatGPT is alleged to sometimes do.*

#### Wrap Up
I have already hashed out my thoughts, and I have clarified that it is okay to disagree with my conclusions (heck, my current opinions aren't even the original ones I formed when I was first introduced to this). Hopefully you got some value in reading through my thought process. Otherwise, instead of rehashing or summarizing, I figure I'll leave you with one final thought:

I once heard someone talking about AI who said something along the lines of "the problem with technologists is they see technology as the cure to all the world's problems." However, I like my computer and would probably be dead if it weren't for antibiotics. So yes, as long as we don't nuke ourselves, I think another tool could be useful in chipping away at the world's problems.
